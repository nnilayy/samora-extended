<h1 align="center">Samora Extended: A Project Demo</h1>

<p align="center">
  <img src="assets/hero.png" alt="Samora Extended Banner" />
</p>

<p align="center">
  <a href="https://www.python.org/"><img src="https://img.shields.io/badge/Python-FFFFFF?logo=python" alt="Python" /></a>
  <a href="https://react.dev/"><img src="https://img.shields.io/badge/ReactJS-FFFFFF?logo=react&logoColor=1142d6" alt="ReactJS" /></a>
  <a href="https://www.mongodb.com/"><img src="https://img.shields.io/badge/MongoDB-FFFFFF?logo=mongodb" alt="MongoDB" /></a>
  <a href="https://pipecat.ai/"><img src="https://img.shields.io/badge/Pipecat-white?logo=data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/Pgo8IURPQ1RZUEUgc3ZnIFBVQkxJQyAiLS8vVzNDLy9EVEQgU1ZHIDIwMDEwOTA0Ly9FTiIKICJodHRwOi8vd3d3LnczLm9yZy9UUi8yMDAxL1JFQy1TVkctMjAwMTA5MDQvRFREL3N2ZzEwLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4wIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciCiB3aWR0aD0iMjAwLjAwMDAwMHB0IiBoZWlnaHQ9IjIwMC4wMDAwMDBwdCIgdmlld0JveD0iMCAwIDIwMC4wMDAwMDAgMjAwLjAwMDAwMCIKIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIG1lZXQiPgoKPGcgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMDAwMDAsMjAwLjAwMDAwMCkgc2NhbGUoMC4xMDAwMDAsLTAuMTAwMDAwKSIKZmlsbD0iIzAwMDAwMCIgc3Ryb2tlPSJub25lIj4KPHBhdGggZD0iTTQ1MCAxNDEwIGMtMTkgLTE5IC0yMCAtMzMgLTIwIC0yNjUgbDAgLTI0NSAtODUgMCAtODUgMCAwIC01NSAwCi01NSAxNDAgMCAxNDAgMCAwIDIxOCAxIDIxNyA2NCAtNzAgNjMgLTcwIDMzMiAwIDMzMSAwIDY0IDcwIDY0IDcwIDEgLTIxNyAwCi0yMTggMTQxIDAgMTQwIDAgLTMgNTMgLTMgNTIgLTgyIDMgLTgzIDMgMCAyNDQgYzAgMjMyIC0xIDI0NiAtMjAgMjY1IC0zOCAzOAotNTEgMjkgLTE3MCAtMTA5IGwtOTUgLTExMCAtMjg1IDAgLTI4NSAwIC02NyA3NyBjLTE1MyAxNzYgLTE2MCAxODAgLTE5OCAxNDJ6Ii8+CjxwYXRoIGQ9Ik03MjUgOTE4IGMtMjcgLTE1IC0zNSAtMjggLTM1IC02MCAwIC02MSA3OCAtOTEgMTIwIC00NiAyNyAyOSAyNSA2NAotNSA5MyAtMjYgMjcgLTQ5IDMxIC04MCAxM3oiLz4KPHBhdGggZD0iTTEyMDUgOTE4IGMtMjcgLTE1IC0zNSAtMjggLTM1IC02MCAwIC02MSA3OCAtOTEgMTIwIC00NiAyNyAyOSAyNQo2NCAtNSA5MyAtMjYgMjcgLTQ5IDMxIC04MCAxM3oiLz4KPHBhdGggZD0iTTI2MCA2MjUgbDAgLTU2IDEzOCAzIDEzNyAzIDMgNTMgMyA1MiAtMTQwIDAgLTE0MSAwIDAgLTU1eiIvPgo8cGF0aCBkPSJNMTQ2MCA2MjUgbDAgLTU2IDEzOCAzIDEzNyAzIDMgNTMgMyA1MiAtMTQwIDAgLTE0MSAwIDAgLTU1eiIvPgo8L2c+Cjwvc3ZnPgo=" alt="Pipecat" /></a>
  <a href="https://www.twilio.com/"><img src="https://img.shields.io/badge/Twilio-FFFFFF?logo=twilio" alt="Twilio" /></a>
</p>

<p align="center">
  <a href="#"><img src="https://img.shields.io/badge/Cerebras-white?logo=data:image/svg+xml;base64,PHN2ZyBmaWxsPSJjdXJyZW50Q29sb3IiIGhlaWdodD0iMWVtIiBzdHlsZT0iZmxleDpub25lO2xpbmUtaGVpZ2h0OjEiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjFlbSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48dGl0bGU+Q2VyZWJyYXM8L3RpdGxlPjxwYXRoIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTE0LjEyMSAyLjcwMWE5LjI5OSA5LjI5OSAwIDAwMCAxOC41OThWMjIuN2MtNS45MSAwLTEwLjctNC43OTEtMTAuNy0xMC43MDFTOC4yMSAxLjI5OSAxNC4xMiAxLjI5OVYyLjd6bTQuNzUyIDMuNjc3QTcuMzUzIDcuMzUzIDAgMTA5LjQyIDE3LjY0M2wtLjkwMSAxLjA3NGE4Ljc1NCA4Ljc1NCAwIDAxLTEuMDgtMTIuMzM0IDguNzU1IDguNzU1IDAgMDExMi4zMzUtMS4wOGwtLjkwMSAxLjA3NXptLTIuMjU1Ljg0NGE1LjQwNyA1LjQwNyAwIDAwLTUuMDQ4IDkuNTYzbC0uNjU2IDEuMjRhNi44MSA2LjgxIDAgMDE2LjM1OC0xMi4wNDNsLS42NTQgMS4yNHpNMTQuMTIgOC41MzlhMy40NiAzLjQ2IDAgMTAwIDYuOTIydjEuNDAyYTQuODYzIDQuODYzIDAgMDEwLTkuNzI2djEuNDAyeiIgZmlsbD0iI0YxNUEyOSIgZmlsbC1ydWxlPSJldmVub2RkIj48L3BhdGg+PHBhdGggZD0iTTE1LjQwNyAxMC44MzZhMi4yNCAyLjI0IDAgMDAtLjUxLS40MDkgMS4wODQgMS4wODQgMCAwMC0uNTQ0LS4xNTJjLS4yNTUgMC0uNDgzLjA0Ny0uNjg0LjE0YTEuNTggMS41OCAwIDAwLS44NC45MTJjLS4wNzQuMjAzLS4xMS40MTYtLjExLjYzMSAwIC4yMTguMDM2LjQzLjExLjYzMWExLjU5NCAxLjU5NCAwIDAwLjg0LjkxM2MuMi4wOTMuNDMuMTQuNjg0LjE0LjIxNiAwIC40MTctLjA0Ni42MDItLjEzNS4xODgtLjA5LjM1LS4yMjUuNDc1LS4zOTJsLjkyOCAxLjAwNmMtLjE0LjE0LS4zLjI2MS0uNDgyLjM2M2EzLjM2NyAzLjM2NyAwIDAxLTEuMDgzLjM4Yy0uMTcuMDI2LS4zMTcuMDQtLjQ0LjA0YTMuMzE1IDMuMzE1IDAgMDEtMS4xODItLjIxIDIuODI1IDIuODI1IDAgMDEtLjk2MS0uNTk3IDIuODE2IDIuODE2IDAgMDEtLjY0NC0uOTI5IDIuOTg3IDIuOTg3IDAgMDEtLjIzOC0xLjIxYzAtLjQ0NC4wOC0uODQ3LjIzOC0xLjIxLjE1LS4zNS4zNjgtLjY2Ni42NDMtLjkyOS4yNzgtLjI2MS42MDUtLjQ2NC45NjItLjU5NmEzLjMxNSAzLjMxNSAwIDAxMS4xODItLjIxYy4zNTUgMCAuNzEyLjA2OCAxLjA3Mi4yMDQuMzYxLjEzOC42ODUuMzYuOTQ0LjY0OWwtLjk2Mi45N3oiPjwvcGF0aD48L3N2Zz4=" alt="Cerebras" /></a>
  <a href="#"><img src="https://img.shields.io/badge/Groq-white?logo=data:image/svg+xml;base64,PHN2ZyBmaWxsPSJjdXJyZW50Q29sb3IiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaGVpZ2h0PSIxZW0iIHN0eWxlPSJmbGV4Om5vbmU7bGluZS1oZWlnaHQ6MSIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMWVtIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjx0aXRsZT5Hcm9xPC90aXRsZT48cGF0aCBkPSJNMTIuMDM2IDJjLTMuODUzLS4wMzUtNyAzLTcuMDM2IDYuNzgxLS4wMzUgMy43ODIgMy4wNTUgNi44NzIgNi45MDggNi45MDdoMi40MnYtMi41NjZoLTIuMjkyYy0yLjQwNy4wMjgtNC4zOC0xLjg2Ni00LjQwOC00LjIzLS4wMjktMi4zNjIgMS45MDEtNC4yOTggNC4zMDgtNC4zMjZoLjFjMi40MDcgMCA0LjM1OCAxLjkxNSA0LjM2NSA0LjI3OHY2LjMwNWMwIDIuMzQyLTEuOTQ0IDQuMjUtNC4zMjMgNC4yNzlhNC4zNzUgNC4zNzUgMCAwMS0zLjAzMy0xLjI1MmwtMS44NTEgMS44MThBNyA3IDAgMDAxMi4wMjkgMjJoLjA5MmMzLjgwMy0uMDU2IDYuODU4LTMuMDgzIDYuODc5LTYuODE2di02LjVDMTguOTA3IDQuOTYzIDE1LjgxNyAyIDEyLjAzNiAyeiI+PC9wYXRoPjwvc3ZnPg==" alt="Groq" /></a>
  <a href="https://openai.com/"><img src="https://img.shields.io/badge/OpenAI-FFFFFF?logo=openai&logoColor=black" alt="OpenAI" /></a>
  <a href="https://ai.google.dev/"><img src="https://img.shields.io/badge/Gemini-FFFFFF?logo=google%20gemini&logoColor=black" alt="Gemini" /></a>
</p>

<p align="center">
  <a href="https://elevenlabs.io/"><img src="https://img.shields.io/badge/ElevenLabs-FFFFFF?logo=elevenlabs&logoColor=black" alt="ElevenLabs" /></a>
  <a href="https://deepgram.com/"><img src="https://img.shields.io/badge/Deepgram-FFFFFF?logo=deepgram&logoColor=black" alt="Deepgram" /></a>
  <a href="#"><img src="https://img.shields.io/badge/Cartesia-white?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQgbWVldCIgdmlld0JveD0iNDg4IDAgMTA3MiAxMDcyIj4KCjxnIHRyYW5zZm9ybT0idHJhbnNsYXRlKDAuMDAwMDAwLDEwNzIuMDAwMDAwKSBzY2FsZSgwLjEwMDAwMCwtMC4xMDAwMDApIiBmaWxsPSIjMDAwMDAwIiBzdHJva2U9Im5vbmUiPgo8cGF0aCBkPSJNODQ1MiA5ODI4IGwzIC04OTMgODkzIC0zIDg5MiAtMiAwIC04OTAgMCAtODkwIC04OTIgLTIgLTg5MyAtMyAwIC0xNzg1IDAgLTE3ODUgODkzIC0zIDg5MiAtMiAwIC04OTAgMCAtODkwIC04OTIgLTIgLTg5MyAtMyAtMyAtODkzIC0yIC04OTIgODk1IDAgODk1IDAgMCA4OTUgMCA4OTUgODk1IDAgODk1IDAgMCAtODk1IDAgLTg5NSA4OTAgMCA4OTAgMCAwIDg5NSAwIDg5NSA4OTUgMCA4OTUgMCAwIDg5MCAwIDg5MCAtODkyIC0yIC04OTMgLTMgLTMgLTg4NyAtMiAtODg4IC04OTAgMCAtODkwIDAgLTIgODg4IC0zIDg4NyAtODkyIDMgLTg5MyAyIDAgMTc5MCAwIDE3OTAgODkzIDIgODkyIDMgMyA4ODggMiA4ODcgODkwIDAgODkwIDAgMiAtODg3IDMgLTg4OCA4OTMgLTMgODkyIC0yIDAgODkwIDAgODkwIC04OTUgMCAtODk1IDAgMCA4OTUgMCA4OTUgLTg5MCAwIC04OTAgMCAwIC04OTUgMCAtODk1IC04OTUgMCAtODk1IDAgMCA4OTUgMCA4OTUgLTg5NSAwIC04OTUgMCAyIC04OTJ6Ii8+CjxwYXRoIGQ9Ik02NjcwIDgwNDAgbDAgLTg5MCAtODk1IDAgLTg5NSAwIDAgLTE3OTAgMCAtMTc5MCA4OTUgMCA4OTUgMCAwIC04OTAgMCAtODkwIDg5MCAwIDg5MCAwIDAgODkwIDAgODkwIC04OTAgMCAtODkwIDAgMCAxNzkwIDAgMTc5MCA4OTAgMCA4OTAgMCAwIDg5MCAwIDg5MCAtODkwIDAgLTg5MCAwIDAgLTg5MHoiLz4KPC9nPgo8L3N2Zz4=" alt="Cartesia" /></a>
</p>

<p align="center">
  <a href="https://www.codefactor.io/repository/github/nnilayy/samora-extended"><img src="https://www.codefactor.io/repository/github/nnilayy/samora-extended/badge" alt="CodeFactor" /></a>
  <a href="https://samora-extended.nnilayy.com/"><img src="https://img.shields.io/website?url=https://samora-extended.nnilayy.com&up_message=Online&down_message=Down&up_color=309c4d&down_color=9c0c0c&label=website" alt="Website Status" /></a>
  <a href="https://github.com/nnilayy/samora-extended/blob/main/LICENSE"><img src="https://img.shields.io/badge/License-MIT-blue.svg?logo=github" alt="License: MIT" /></a>
  <a href="https://github.com/nnilayy/samora-extended/stargazers"><img src="https://img.shields.io/github/stars/nnilayy/samora-extended?style=social" alt="GitHub Stars" /></a>
</p>

## Overview

## Observations/Gaps in Current Agents and Corresponding Fixes/Improvements  Introduced
While testing Samora’s demos and exploring agent behavior, the following observations were made. For each, a corresponding feature or fix was introduced to address the gap/issue.

| **Features**                                                      | **Observations in Current Samora Agents**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | **Improvements Introduced**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| ----------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Rolling Summarizer Context Window Manager**                     | `Observation`: - <br><br>`Example`: - <br><br>`Note & Motivation`: -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | `Fix:` The `RollingSummarizerContextManager` class is implemented from scratch as a subclass of `FrameProcessor`, specifically designed to monitor and manage the size of the `LLMContext`. It is configured with two key parameters, `threshold` and `keep_recent`. The `threshold` (e.g., 100) defines the maximum number of messages allowed before summarization is triggered, while `keep_recent` (e.g., 20) determines how many of the most recent messages are preserved in full. Once the number of messages exceeds the threshold, the `_run_summarization()` function is triggered to summarize the middle section of the conversation using the active `llm_service` through its `run_inference()` method, keeping the system prompt and latest messages intact. The summarization process executes asynchronously in a separate coroutine, running independently of the main pipeline to ensure that no frame propagation, response generation, or user interaction is ever blocked. Before the context manager updates the `LLMContext` with the summarized and latest messages, it waits for a safe synchronization point, specifically when an `LLMFullResponseEndFrame` is received, ensuring that no active LLM generation or streaming operation is interrupted. At that point, `_apply_pending_merge()` performs an in-place context update using `set_messages()`, replacing older messages with the summarized block and merging any new messages received during summarization. This design keeps the context compact and efficient while maintaining uninterrupted conversation flow, reducing both token load and inference latency. |
| **Put Agent on Hold and Resume Call When Back**                   | `Observation`: In certain service or booking calls, users may need to pause the conversation to think, consult with someone nearby, or discuss details with another person in the same room. For example, while making a reservation, a person might check with their partner about changing dates or confirming details. Similarly, in service-related calls, a family member might answer and ask the agent to stay on the line until the intended person arrives. In these cases, the user implicitly expects the agent to stay on call silently until they return. However, as soon as any background conversation begins, the agent mistakenly assumes the speech is directed at it and starts responding, interrupting the user instead of remaining on hold. <br><br>`Example`: All agents currently lack the ability to stay on hold. When asked to "please stay on the call" or "hold on a second", they reply with phrases like "Sure, let me know when you're back", but immediately start speaking again as soon as they detect background speech, instead of waiting silently until the user explicitly says they are back. | `Fix:` Implemented the `put_on_hold` tool integrated with a custom `HoldWakeProcessor` class to manage real-time hold and resume logic within the pipeline. When invoked, the tool sets `is_on_hold = True` through `hold_wake_processor.set_hold(True)` and queues a `TTSSpeakFrame` response: "No problem! I'll wait right here. Just say I'm back when you're ready to continue." The `HoldWakeProcessor` class extends `FrameProcessor` and intercepts all incoming `TranscriptionFrame` objects while the agent is on hold, filtering them through regex-based wake phrase detection (`WAKE_PROMPTS` such as "I'm back", "let's continue", "you there"). Non-matching frames are silently dropped to suppress unintended speech detection. Once a valid wake phrase is detected, the class resets `is_on_hold` to `False`, pushes the transcription downstream, and resumes normal LLM processing through the `LLMContext` and `LLMResponseAggregator`. The processor also cooperates with the `UserIdleProcessor`, ensuring idle triggers are suspended during hold mode, preventing false "are you still there?" prompts. This implementation allows the agent to remain silently connected and resume naturally once the user returns, maintaining conversational continuity and context integrity across interruptions.                                                                                                                                                                                                                                                                                                                      |
| **Improved Call Ending for Agents**                               | `Observation`: When asked to end the call, some agents say their closing lines and goodbye but stay idle afterward, triggering the user-idle detection instead of properly ending the conversation. Others disconnect immediately without confirming if the user needs more help, resulting in abrupt endings.<br><br>`Example`: For the US Accent, Real Estate and Non-Profit agents say their goodbyes but then remain silent and ask "Hey, are you there?", lingering on the line instead of ending the call. Meanwhile, the Hospitality and Service Feedback agents cut the call instantly without any closure or offer for further help while closing out.                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | `Fix:` Implemented the `end_call` tool and added instructions and examples for the LLM to observe and handle call endings correctly. The LLM now follows a two-step process: when the user asks to end the call or the conversation seems to be concluding, the agent first responds politely, for example:<br>**Caller:** "Actually, I have to go."<br>**Agent:** "No problem at all! Before you go, is there anything quick I can help with?"<br>If the user confirms, the `end_call` tool is triggered, which outputs a `TTSSpeakFrame` saying "It was great talking with you! Feel free to reach out anytime. Take care!", followed by an `EndFrame()` that gracefully disconnects the pipeline, closing the call or web agent on the frontend.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| **Multilingual Language Support**                                 | `Observation`: The demo agents currently support conversations only in English and Hindi. If a user is not comfortable in either of these languages, the system fails to handle inputs in other languages or when users switch mid-conversation to their native language for better clarity or comfort.<br><br>`Example`: When any agent with a US accent is asked if they can speak another language, they respond that they can only speak and support English. If a user greets in another language such as Spanish ("Hola") or Tamil ("Vanakkam"), the agent either stays silent or misinterprets the phrase instead of responding naturally.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | `Fix:` Added STT and TTS providers with multilingual language support, along with improved instructions and examples added to the LLM system prompt to ensure language switching happens only after explicit user confirmation, maintaining consistent and controlled conversation flow. Now, if a user speaks in Japanese (e.g., "日本語で話せますか?"), the agent detects the language and replies naturally with a confirmation prompt like "I see you are speaking in Japanese, would you like me to switch to Japanese?". Once confirmed, the conversation continues in Japanese. If a user asks if they can speak in French, the same confirmation process occurs, and the conversation then flows in French until the user switches back to English or explicitly requests a language change.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| **Fixed Tool Call/System Prompt Spillover to LLM and TTS Output** | `Observation`: There are leaks of tool call syntax or internal system prompts into LLM and TTS output, causing the agent to utter unintended technical phrases.<br><br>`Example`: For the US Accent, Real Estate Agent, when the user interrupts by saying "stop" or "pause", the agent responds with internal terms like "break time" followed by a time duration, which then persists across subsequent conversations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | `Fix:` The same behavior was observed during local testing on my system, when using smaller models from providers such as Cerebras and Groq (e.g., 70B or 8B variants). It was mitigated by switching to larger, more stable models with much better tool calling from providers like OpenAI and Gemini, which resolved the spillover issue and restored proper conversation flow.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |


## Features

To address the observed gaps and demo a more holistic experience, a scenario was built around a Voice AI Agent serving as a Hotel Front Desk concierge, similar to other Samora agents in hospitality and service domains.

In addition to addressing the observed gaps, the following features are part of the Samora-Extended Demo, showcasing a complete, fully functional concierge agent.

* **Live MongoDB Database Access**: Connected to a MongoDB instance, the agent handles hotel tasks like availability checks, bookings, updates, cancellations, and special requests. All write operations are executed in real time using custom tools for each database function, with updates reflected instantly and confirmed once complete. The agent supports eight dedicated database tools: `check_availability`, `book_room`, `lookup_booking`, `update_booking`, `cancel_booking`, `add_special_request`, `get_amenities`, and `get_pricing`.

* **Agent Configuration**: A frontend Settings panel allows dynamic selection of LLM, TTS, and STT models at runtime. Any combination of one model from each category can be selected without redeployment.

  * **STT Models**: Deepgram (`nova-3`), ElevenLabs (`scribe_v2_realtime`)
  * **LLM Models**: OpenAI (`gpt-4o-mini`), Cerebras (`llama-3.3-70b`), Groq (`llama-3.3-70b-versatile`), Google (`gemini-2.5-flash`)
  * **TTS Models**: Deepgram (`aura-2-theia-en`), Cartesia (Voice ID `248be419-c632-4f23-adf1-5324ed7dbf1d`)

* **Context Configuration**: A frontend UI to configure when the Rolling Context Summarizer activates and how much recent conversation to retain. This helps manage token usage in the long run and cut down on costs, while maintaining continuity in long conversations. The following parameters are exposed in the Settings panel:

  * `context_threshold`: Number of messages after which summarization is triggered
  * `context_keep_recent`: Number of most recent messages to keep in the latest context without being summarized

* **Web Deployment**: Backend hosted on Pipecat Cloud and frontend on Vercel. The agent is accessible via both the website and a Twilio phone number, supporting smooth interaction across browser and phone.

  * **Web Site for Web Agent Access**: [https://samora-extended.nnilayy.com/](https://samora-extended.nnilayy.com/)
  * **Twilio Phone Number for Phone-based Agent Interaction**: +1 (520) 652-1762



## License

This project is licensed under the **MIT License** - checkout the [LICENSE](LICENSE) for more details.

## Support & Contact


If you have any questions or need further assistance, feel free to reach out:

- **GitHub Issues**: [Issues Page](https://github.com/nnilayy/samora-extended/issues/new)
- **Email**: nnilayy.work@gmail.com

---

<p align="center">Made with ❤️ by <a href="https://github.com/nnilayy">nnilayy</a></p>